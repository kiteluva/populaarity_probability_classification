---
title: "spotify classification analysis"
author: "kite-luva"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)


##############################
### CODE FOR MULTI-CLASS AUC & ROC WITH PDF OUTPUT (USING ROC FOR TRAINING) ###
##############################

# Load necessary libraries
library(tidyverse)
library(caret)
library(ranger)
library(lubridate)
library(pROC)
library(readr)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(knitr)
library(rmarkdown)

# ------------------------
# 1. Load & Clean the Data
# ------------------------

# Load your dataset (adjust the file path accordingly)
spotify_charts_2024 <- read_csv("~/school docs/universal_top_spotify_songs.new.csv")

# Convert date columns and calculate difference in days
spotify_charts_2024 <- spotify_charts_2024 %>%
  mutate(snapshot_date = ymd(snapshot_date),
         album_release_date = ymd(album_release_date),
         days_out = as.numeric(snapshot_date - album_release_date))

# Remove duplicates based on the spotify_id column while retaining all columns
spotify_charts_2024 <- spotify_charts_2024 %>%
  distinct(spotify_id, .keep_all = TRUE)

# Remove unneeded columns
spotify_charts_2024 <- spotify_charts_2024 %>%
  select(-country, -snapshot_date, -name, -artists, -album_name, -album_release_date, -spotify_id)

# Convert 'is_explicit' (boolean) to integer
spotify_charts_2024$is_explicit <- as.integer(spotify_charts_2024$is_explicit)

# Handle missing values in numeric columns only
numeric_cols <- sapply(spotify_charts_2024, is.numeric)
spotify_charts_2024[numeric_cols] <- lapply(spotify_charts_2024[numeric_cols],
                                            function(x) ifelse(is.na(x), median(x, na.rm = TRUE), x))

# Standardize 'duration_ms' to minutes, then remove the original column
spotify_charts_2024 <- spotify_charts_2024 %>%
  mutate(duration_min = duration_ms / 60000) %>%
  select(-duration_ms)

# ------------------------
# 2. Prepare Data for Multi-Class Classification (No Medium)
# ------------------------

#remove popularity 0
spotify_charts_2024 <- spotify_charts_2024 %>%
  filter(popularity != 0)

# Define popularity thresholds (adjusted to remove Medium)
very_high_threshold <- 75
high_threshold <- 50
very_low_threshold <- 25

spotify_charts_2024 <- spotify_charts_2024 %>%
  mutate(popularity_level = case_when(
    popularity >= very_high_threshold ~ "very_high",
    popularity < very_low_threshold ~ "very_low",
    popularity >= high_threshold ~ "high",
    popularity < high_threshold ~ "low"
  )) %>%
  mutate(popularity_level = factor(popularity_level))
# Define feature columns
feature_columns <- c("daily_rank", "duration_min","daily_movement","weekly_movement",
                     "days_out", "is_explicit", "mode", "danceability", "energy", "loudness",
                     "speechiness", "acousticness", "instrumentalness", "time_signature",
                     "liveness", "valence", "key", "tempo")

# Create a dataset with predictors and the target variable
class_data <- spotify_charts_2024 %>%
  select(all_of(feature_columns), popularity_level)

# Split the dataset into training (80%) and testing sets
set.seed(50)
trainIndex <- createDataPartition(class_data$popularity_level, p = 0.8, list = FALSE)
train_data  <- class_data[trainIndex, ]
test_data   <- class_data[-trainIndex, ]

# ------------------------
# 3. Train the Random Forest Classifier (Multi-Class) using ROC
# ------------------------

# Custom summary function for multi-class ROC (using pROC package)
multiClassSummary <- function (data, lev = NULL, model = NULL) {
  # Check if the number of classes is greater than 2
  if (length(lev) > 2) {
    # One-vs-all ROC calculation
    rocs <- pROC::multiclass.roc(data$obs, data[, lev])
    auc <- pROC::auc(rocs)
    names(auc) <- "AUC"
    # Overall accuracy
    accuracy <- mean(data$obs == data$pred)
    names(accuracy) <- "Accuracy"
    return(c(AUC = auc, Accuracy = accuracy))
  } else {
    # For binary classification, use default ROC summary
    return(defaultSummary(data, lev, model))
  }
}

# Modified trainControl
trainControl_multi_roc <- trainControl(method = "cv",
                                       number = 5,
                                       allowParallel = TRUE,
                                       summaryFunction = multiClassSummary,
                                       classProbs = TRUE,
                                       savePredictions = TRUE)

# Modified train call
rf_model_multi <- train(popularity_level ~ .,
                        data = train_data,
                        method = "ranger",
                        trControl = trainControl_multi_roc,
                        tuneGrid = expand.grid(mtry = c(5, 7, 9, 11),
                                               min.node.size = c(1, 3, 5),
                                               splitrule = "gini"),
                        num.trees = 200,
                        metric = "AUC")

model_results_roc <- rf_model_multi$results
print(model_results_roc)

# ------------------------
# 4. Compute and Plot Multi-Class ROC Curves on Test Set
# ------------------------

test_data_predictors <- test_data %>%
  select(-popularity_level)

# Get predicted probabilities on the test set
rf_pred_probs_multi <- predict(rf_model_multi, newdata = test_data_predictors, type = "prob")

# Get class levels from the model
class_levels <- levels(rf_model_multi$levels)
roc_objects_multi <- list()

# Get class levels from the model
class_levels <- levels(rf_model_multi$levels)
if(length(class_levels) == 0) {
  class_levels <- levels(train_data$popularity_level)
}
for (i in 1:length(class_levels)) {
  current_class <- class_levels[i]
  binary_response <- ifelse(test_data$popularity_level == current_class, 1, 0)
  predictor <- rf_pred_probs_multi[, current_class]
  if (current_class %in% colnames(rf_pred_probs_multi)) {
    roc_objects_multi[[current_class]] <- roc(response = binary_response, predictor = predictor)
  } else {
    cat(paste("Warning: Class '", current_class, "' not found in prediction probabilities.\n"))
  }
}

# Create the multi-class ROC plot with a legend
if (length(roc_objects_multi) > 0) {
  plot(roc_objects_multi[[1]], col = 1, main = "One-vs-Rest ROC Curves (Test Set)",
       xlab = "False Positive Rate (1 - Specificity)", ylab = "True Positive Rate (Sensitivity)")
  for (i in 2:length(roc_objects_multi)) {
    plot(roc_objects_multi[[i]], add = TRUE, col = i)
  }
  # Add a legend
  legend("bottomright", legend = names(roc_objects_multi), col = 1:length(roc_objects_multi), lty = 1)
  roc_plot_multi <- recordPlot()
} else {
  roc_plot_multi <- NULL
  cat("Warning: No ROC curves could be generated.\n")
}

# Calculate and store AUC for each class
auc_values_multi <- sapply(roc_objects_multi, auc)
auc_table <- data.frame(Class = names(auc_values_multi), AUC = auc_values_multi)
print("AUC for each class (One-vs-Rest on Test Set):")
print(auc_table)

################################################################################
# Create an empty list to store ROC curves for different tuning parameters
roc_list_multi <- list()
# Loop through each tuning parameter combination
for(i in 1:nrow(rf_model_multi$results)){
  # extract parameters
  mtry_val <- rf_model_multi$results$mtry[i]
  node_size_val <- rf_model_multi$results$min.node.size[i]
  # make predictions using test data
  predictions_multi <- predict(rf_model_multi, newdata = test_data_predictors, type= "prob")
  
  for (j in 1:length(class_levels)) {
    current_class <- class_levels[j]
    binary_response <- ifelse(test_data$popularity_level == current_class, 1, 0)
    if (current_class %in% colnames(predictions_multi)) {
      predictor <- predictions_multi[, current_class]
      roc_curve <- roc(binary_response, predictor)
      roc_list_multi[[paste("mtry=", mtry_val, " node.size=", node_size_val, "-", current_class)]] <- roc_curve
    } else {
      cat(paste("Warning: Class '", current_class, "' not found in predictions for tuning parameter set.\n"))
    }
  }
}

###
roc_data_multi_plot_tuning <- do.call(rbind, lapply(names(roc_list_multi), function(label) {
  parts <- strsplit(label, " - ")[[1]]
  combination <- parts[1]
  class_name <- parts[2]
  if (length(roc_list_multi[[label]]$specificities) > 0) {
    data.frame(
      Specificity = roc_list_multi[[label]]$specificities,
      Sensitivity = roc_list_multi[[label]]$sensitivities,
      Combination = combination,
      Class = class_name
    )
  } else {
    NULL # Return NULL for empty ROC objects
  }
})) %>%
  filter(!is.null(Specificity))

# Plot ROC curves with facets for each combination and color for each class
roc_plot_tuning <- ggplot(roc_data_multi_plot_tuning, aes(x = Specificity, y = Sensitivity, color = Class)) +
  geom_line() +
  labs(
    title = "One-vs-Rest ROC Curves for Different mtry and min.node.size Combinations",
    x = "1-Specificity", y = "Sensitivity", color = "Popularity Level"
  ) +
  theme_minimal() +
  facet_wrap(~Combination) +
  scale_x_reverse()

# Make predictions on the test set
rf_predictions_class <- predict(rf_model_multi, newdata = test_data_predictors)

# Create confusion matrix
confusion_matrix <- confusionMatrix(rf_predictions_class, test_data$popularity_level)
confusion_matrix_table <- as.data.frame(confusion_matrix$table)

```